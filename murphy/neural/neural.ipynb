{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d089d808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score, log_loss\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(20712124)\n",
    "\n",
    "data_train = pd.read_csv(\"../preprocessing/preprocesed_variables_train.csv\")\n",
    "important_variables = pd.read_csv(\"../preprocessing/important.csv\")\n",
    "\n",
    "X = data_train.drop(columns=['health'])\n",
    "X = X[important_variables['0']]\n",
    "y = data_train['health']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=20712124, stratify = y)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "X_train = X_train.drop(X_train[to_drop], axis=1)\n",
    "X_test = X_test.drop(X_test[to_drop], axis=1)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "temp_m = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = temp_m.transform(X_train)\n",
    "X_new_test = temp_m.transform(X_test)\n",
    "cols = temp_m.get_support(indices=True)\n",
    "X_new = pd.DataFrame(X_new, columns = cols)\n",
    "X_new_test = pd.DataFrame(X_new_test, columns = cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e0a701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.5       , ..., 0.46666667, 0.83333333,\n",
       "        1.        ],\n",
       "       [0.25      , 0.25      , 0.5       , ..., 0.4       , 0.83333333,\n",
       "        0.77777778],\n",
       "       [0.5       , 0.25      , 0.75      , ..., 0.86666667, 1.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.5       , 0.5       , 0.25      , ..., 0.93333333, 0.33333333,\n",
       "        0.22222222],\n",
       "       [0.25      , 0.25      , 0.5       , ..., 0.26666667, 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 0.5       , ..., 0.46666667, 0.33333333,\n",
       "        0.88888889]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "033b820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(fn):\n",
    "    return lambda x: -fn(x)\n",
    "\n",
    "score_acc = make_scorer(accuracy_score)\n",
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs,\n",
    "             layers1, layers2, dropout, dropout_rate ):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(learning_rate=learning_rate), 'SGD':SGD(learning_rate=learning_rate),\n",
    "                 'RMSprop':RMSprop(learning_rate=learning_rate), 'Adadelta':Adadelta(learning_rate=learning_rate),\n",
    "                 'Adagrad':Adagrad(learning_rate=learning_rate), 'Adamax':Adamax(learning_rate=learning_rate),\n",
    "                 'Nadam':Nadam(learning_rate=learning_rate), 'Ftrl':Ftrl(learning_rate=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', 'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerL[round(optimizer)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    def nn_cl_fun():\n",
    "        opt = optimizerD[optimizer]\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(len(X_new.columns,))) # input with 934 \n",
    "        x = inputs\n",
    "        for i in range(layers1):\n",
    "            x = Dense(neurons, activation=activation)(x)\n",
    "        if dropout < 0.5:\n",
    "            x = Dropout(dropout_rate, seed=123)(x)\n",
    "        for i in range(layers2):\n",
    "            x = Dense(neurons, activation=activation)(x)\n",
    "        outputs = Dense(5, activation='softmax')(x) #output\n",
    "        nn = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"base_model\")\n",
    "\n",
    "        nn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy']) \n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(model=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_new, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658a994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.4025  \u001b[0m | \u001b[0m 0.2174  \u001b[0m | \u001b[0m 427.8   \u001b[0m | \u001b[0m 0.4245  \u001b[0m | \u001b[0m 0.2534  \u001b[0m | \u001b[0m 4.321   \u001b[0m | \u001b[0m 2.122   \u001b[0m | \u001b[0m 2.671   \u001b[0m | \u001b[0m 0.2495  \u001b[0m | \u001b[0m 29.78   \u001b[0m | \u001b[0m 3.73    \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.4509  \u001b[0m | \u001b[95m 0.3565  \u001b[0m | \u001b[95m 420.9   \u001b[0m | \u001b[95m 0.1853  \u001b[0m | \u001b[95m 0.03251 \u001b[0m | \u001b[95m 18.94   \u001b[0m | \u001b[95m 2.979   \u001b[0m | \u001b[95m 2.812   \u001b[0m | \u001b[95m 0.05986 \u001b[0m | \u001b[95m 53.57   \u001b[0m | \u001b[95m 3.61    \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.4482  \u001b[0m | \u001b[0m 0.1727  \u001b[0m | \u001b[0m 494.0   \u001b[0m | \u001b[0m 0.8176  \u001b[0m | \u001b[0m 0.1008  \u001b[0m | \u001b[0m 15.93   \u001b[0m | \u001b[0m 2.373   \u001b[0m | \u001b[0m 2.006   \u001b[0m | \u001b[0m 0.0832  \u001b[0m | \u001b[0m 52.85   \u001b[0m | \u001b[0m 3.506   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.4292  \u001b[0m | \u001b[0m 0.2395  \u001b[0m | \u001b[0m 460.4   \u001b[0m | \u001b[0m 0.1051  \u001b[0m | \u001b[0m 0.1146  \u001b[0m | \u001b[0m 6.48    \u001b[0m | \u001b[0m 2.89    \u001b[0m | \u001b[0m 2.981   \u001b[0m | \u001b[0m 0.02738 \u001b[0m | \u001b[0m 56.17   \u001b[0m | \u001b[0m 3.731   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.4146  \u001b[0m | \u001b[0m 0.297   \u001b[0m | \u001b[0m 463.0   \u001b[0m | \u001b[0m 0.5818  \u001b[0m | \u001b[0m 0.006132\u001b[0m | \u001b[0m 18.28   \u001b[0m | \u001b[0m 2.545   \u001b[0m | \u001b[0m 2.769   \u001b[0m | \u001b[0m 0.0827  \u001b[0m | \u001b[0m 35.01   \u001b[0m | \u001b[0m 3.841   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4462  \u001b[0m | \u001b[0m 0.39    \u001b[0m | \u001b[0m 488.5   \u001b[0m | \u001b[0m 0.3595  \u001b[0m | \u001b[0m 0.1797  \u001b[0m | \u001b[0m 28.13   \u001b[0m | \u001b[0m 2.34    \u001b[0m | \u001b[0m 2.178   \u001b[0m | \u001b[0m 0.07893 \u001b[0m | \u001b[0m 26.57   \u001b[0m | \u001b[0m 3.702   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.4358  \u001b[0m | \u001b[0m 0.1505  \u001b[0m | \u001b[0m 459.3   \u001b[0m | \u001b[0m 0.6299  \u001b[0m | \u001b[0m 0.04278 \u001b[0m | \u001b[0m 67.5    \u001b[0m | \u001b[0m 2.946   \u001b[0m | \u001b[0m 2.602   \u001b[0m | \u001b[0m 0.1225  \u001b[0m | \u001b[0m 37.71   \u001b[0m | \u001b[0m 3.582   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.4484  \u001b[0m | \u001b[0m 0.1107  \u001b[0m | \u001b[0m 424.7   \u001b[0m | \u001b[0m 0.1736  \u001b[0m | \u001b[0m 0.29    \u001b[0m | \u001b[0m 69.08   \u001b[0m | \u001b[0m 2.598   \u001b[0m | \u001b[0m 2.731   \u001b[0m | \u001b[0m 0.1087  \u001b[0m | \u001b[0m 28.22   \u001b[0m | \u001b[0m 3.685   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.4552  \u001b[0m | \u001b[95m 0.2035  \u001b[0m | \u001b[95m 408.8   \u001b[0m | \u001b[95m 0.528   \u001b[0m | \u001b[95m 0.2976  \u001b[0m | \u001b[95m 30.86   \u001b[0m | \u001b[95m 2.336   \u001b[0m | \u001b[95m 2.805   \u001b[0m | \u001b[95m 0.2288  \u001b[0m | \u001b[95m 35.96   \u001b[0m | \u001b[95m 3.754   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.4428  \u001b[0m | \u001b[0m 0.2162  \u001b[0m | \u001b[0m 429.7   \u001b[0m | \u001b[0m 0.1108  \u001b[0m | \u001b[0m 0.09379 \u001b[0m | \u001b[0m 35.07   \u001b[0m | \u001b[0m 2.659   \u001b[0m | \u001b[0m 2.254   \u001b[0m | \u001b[0m 0.1959  \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 3.763   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.4403  \u001b[0m | \u001b[0m 0.3113  \u001b[0m | \u001b[0m 478.0   \u001b[0m | \u001b[0m 0.6103  \u001b[0m | \u001b[0m 0.0927  \u001b[0m | \u001b[0m 51.45   \u001b[0m | \u001b[0m 2.86    \u001b[0m | \u001b[0m 2.625   \u001b[0m | \u001b[0m 0.2949  \u001b[0m | \u001b[0m 59.18   \u001b[0m | \u001b[0m 3.567   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.4158  \u001b[0m | \u001b[0m 0.009271\u001b[0m | \u001b[0m 416.1   \u001b[0m | \u001b[0m 0.9235  \u001b[0m | \u001b[0m 0.2861  \u001b[0m | \u001b[0m 18.35   \u001b[0m | \u001b[0m 2.361   \u001b[0m | \u001b[0m 2.549   \u001b[0m | \u001b[0m 0.08883 \u001b[0m | \u001b[0m 41.12   \u001b[0m | \u001b[0m 3.778   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.452   \u001b[0m | \u001b[0m 0.2001  \u001b[0m | \u001b[0m 471.6   \u001b[0m | \u001b[0m 0.526   \u001b[0m | \u001b[0m 0.000419\u001b[0m | \u001b[0m 30.84   \u001b[0m | \u001b[0m 2.492   \u001b[0m | \u001b[0m 2.403   \u001b[0m | \u001b[0m 0.1127  \u001b[0m | \u001b[0m 42.52   \u001b[0m | \u001b[0m 3.678   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.4226  \u001b[0m | \u001b[0m 0.03617 \u001b[0m | \u001b[0m 427.4   \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.007963\u001b[0m | \u001b[0m 6.72    \u001b[0m | \u001b[0m 2.283   \u001b[0m | \u001b[0m 2.582   \u001b[0m | \u001b[0m 0.2974  \u001b[0m | \u001b[0m 59.74   \u001b[0m | \u001b[0m 3.897   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.442   \u001b[0m | \u001b[0m 0.04402 \u001b[0m | \u001b[0m 466.4   \u001b[0m | \u001b[0m 0.524   \u001b[0m | \u001b[0m 0.05194 \u001b[0m | \u001b[0m 68.12   \u001b[0m | \u001b[0m 2.242   \u001b[0m | \u001b[0m 2.999   \u001b[0m | \u001b[0m 0.179   \u001b[0m | \u001b[0m 31.41   \u001b[0m | \u001b[0m 3.655   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.4457  \u001b[0m | \u001b[0m 0.07587 \u001b[0m | \u001b[0m 441.1   \u001b[0m | \u001b[0m 0.5947  \u001b[0m | \u001b[0m 0.215   \u001b[0m | \u001b[0m 37.11   \u001b[0m | \u001b[0m 2.31    \u001b[0m | \u001b[0m 2.577   \u001b[0m | \u001b[0m 0.1381  \u001b[0m | \u001b[0m 37.59   \u001b[0m | \u001b[0m 3.629   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.4508  \u001b[0m | \u001b[0m 0.08328 \u001b[0m | \u001b[0m 445.1   \u001b[0m | \u001b[0m 0.4918  \u001b[0m | \u001b[0m 0.2697  \u001b[0m | \u001b[0m 53.6    \u001b[0m | \u001b[0m 2.77    \u001b[0m | \u001b[0m 2.375   \u001b[0m | \u001b[0m 0.1097  \u001b[0m | \u001b[0m 47.93   \u001b[0m | \u001b[0m 3.784   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.4487  \u001b[0m | \u001b[0m 0.04542 \u001b[0m | \u001b[0m 413.3   \u001b[0m | \u001b[0m 0.456   \u001b[0m | \u001b[0m 0.04792 \u001b[0m | \u001b[0m 69.39   \u001b[0m | \u001b[0m 2.838   \u001b[0m | \u001b[0m 2.52    \u001b[0m | \u001b[0m 0.0733  \u001b[0m | \u001b[0m 29.72   \u001b[0m | \u001b[0m 3.892   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.4455  \u001b[0m | \u001b[0m 0.2828  \u001b[0m | \u001b[0m 486.0   \u001b[0m | \u001b[0m 0.3872  \u001b[0m | \u001b[0m 0.07525 \u001b[0m | \u001b[0m 24.36   \u001b[0m | \u001b[0m 2.857   \u001b[0m | \u001b[0m 2.473   \u001b[0m | \u001b[0m 0.2024  \u001b[0m | \u001b[0m 53.2    \u001b[0m | \u001b[0m 3.601   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.431   \u001b[0m | \u001b[0m 0.03183 \u001b[0m | \u001b[0m 473.3   \u001b[0m | \u001b[0m 0.9614  \u001b[0m | \u001b[0m 0.2861  \u001b[0m | \u001b[0m 37.35   \u001b[0m | \u001b[0m 2.632   \u001b[0m | \u001b[0m 2.733   \u001b[0m | \u001b[0m 0.2717  \u001b[0m | \u001b[0m 30.68   \u001b[0m | \u001b[0m 3.662   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.4538  \u001b[0m | \u001b[0m 0.1668  \u001b[0m | \u001b[0m 469.6   \u001b[0m | \u001b[0m 0.4248  \u001b[0m | \u001b[0m 0.2574  \u001b[0m | \u001b[0m 61.59   \u001b[0m | \u001b[0m 2.07    \u001b[0m | \u001b[0m 2.302   \u001b[0m | \u001b[0m 0.2941  \u001b[0m | \u001b[0m 26.25   \u001b[0m | \u001b[0m 3.697   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.4509  \u001b[0m | \u001b[0m 0.381   \u001b[0m | \u001b[0m 481.1   \u001b[0m | \u001b[0m 0.2943  \u001b[0m | \u001b[0m 0.1789  \u001b[0m | \u001b[0m 33.32   \u001b[0m | \u001b[0m 2.592   \u001b[0m | \u001b[0m 2.894   \u001b[0m | \u001b[0m 0.1707  \u001b[0m | \u001b[0m 42.25   \u001b[0m | \u001b[0m 3.628   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.4537  \u001b[0m | \u001b[0m 0.1053  \u001b[0m | \u001b[0m 454.2   \u001b[0m | \u001b[0m 0.08226 \u001b[0m | \u001b[0m 0.1907  \u001b[0m | \u001b[0m 58.16   \u001b[0m | \u001b[0m 2.955   \u001b[0m | \u001b[0m 2.685   \u001b[0m | \u001b[0m 0.1516  \u001b[0m | \u001b[0m 41.99   \u001b[0m | \u001b[0m 3.887   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.4537  \u001b[0m | \u001b[0m 0.08454 \u001b[0m | \u001b[0m 441.2   \u001b[0m | \u001b[0m 0.9897  \u001b[0m | \u001b[0m 0.008524\u001b[0m | \u001b[0m 51.69   \u001b[0m | \u001b[0m 2.025   \u001b[0m | \u001b[0m 2.321   \u001b[0m | \u001b[0m 0.03132 \u001b[0m | \u001b[0m 27.13   \u001b[0m | \u001b[0m 3.545   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.3848  \u001b[0m | \u001b[0m 0.06771 \u001b[0m | \u001b[0m 462.8   \u001b[0m | \u001b[0m 0.4384  \u001b[0m | \u001b[0m 0.2493  \u001b[0m | \u001b[0m 20.31   \u001b[0m | \u001b[0m 2.19    \u001b[0m | \u001b[0m 2.712   \u001b[0m | \u001b[0m 0.2589  \u001b[0m | \u001b[0m 44.57   \u001b[0m | \u001b[0m 3.782   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.4351  \u001b[0m | \u001b[0m 0.242   \u001b[0m | \u001b[0m 455.9   \u001b[0m | \u001b[0m 0.8604  \u001b[0m | \u001b[0m 0.2759  \u001b[0m | \u001b[0m 61.77   \u001b[0m | \u001b[0m 2.254   \u001b[0m | \u001b[0m 2.878   \u001b[0m | \u001b[0m 0.1362  \u001b[0m | \u001b[0m 50.53   \u001b[0m | \u001b[0m 3.665   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.4525  \u001b[0m | \u001b[0m 0.07633 \u001b[0m | \u001b[0m 470.6   \u001b[0m | \u001b[0m 0.2406  \u001b[0m | \u001b[0m 0.2554  \u001b[0m | \u001b[0m 60.04   \u001b[0m | \u001b[0m 2.525   \u001b[0m | \u001b[0m 2.386   \u001b[0m | \u001b[0m 0.1814  \u001b[0m | \u001b[0m 29.81   \u001b[0m | \u001b[0m 3.823   \u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m 0.4569  \u001b[0m | \u001b[95m 0.3863  \u001b[0m | \u001b[95m 478.0   \u001b[0m | \u001b[95m 0.2393  \u001b[0m | \u001b[95m 0.2602  \u001b[0m | \u001b[95m 58.95   \u001b[0m | \u001b[95m 2.064   \u001b[0m | \u001b[95m 2.231   \u001b[0m | \u001b[95m 0.181   \u001b[0m | \u001b[95m 29.81   \u001b[0m | \u001b[95m 3.771   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.4528  \u001b[0m | \u001b[0m 0.3969  \u001b[0m | \u001b[0m 428.6   \u001b[0m | \u001b[0m 0.7609  \u001b[0m | \u001b[0m 0.01396 \u001b[0m | \u001b[0m 26.61   \u001b[0m | \u001b[0m 2.945   \u001b[0m | \u001b[0m 2.637   \u001b[0m | \u001b[0m 0.1845  \u001b[0m | \u001b[0m 57.49   \u001b[0m | \u001b[0m 3.573   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.444   \u001b[0m | \u001b[0m 0.007129\u001b[0m | \u001b[0m 419.0   \u001b[0m | \u001b[0m 0.5219  \u001b[0m | \u001b[0m 0.1487  \u001b[0m | \u001b[0m 58.43   \u001b[0m | \u001b[0m 2.859   \u001b[0m | \u001b[0m 2.213   \u001b[0m | \u001b[0m 0.1368  \u001b[0m | \u001b[0m 39.76   \u001b[0m | \u001b[0m 3.522   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.4488  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 451.1   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 49.85   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 34.83   \u001b[0m | \u001b[0m 3.9     \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.4501  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 440.6   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2544  \u001b[0m | \u001b[0m 60.66   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.2585  \u001b[0m | \u001b[0m 36.63   \u001b[0m | \u001b[0m 3.9     \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.4491  \u001b[0m | \u001b[0m 0.3943  \u001b[0m | \u001b[0m 404.5   \u001b[0m | \u001b[0m 0.1112  \u001b[0m | \u001b[0m 0.1394  \u001b[0m | \u001b[0m 41.5    \u001b[0m | \u001b[0m 2.524   \u001b[0m | \u001b[0m 2.933   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 32.96   \u001b[0m | \u001b[0m 3.634   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.4016  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 415.6   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 29.64   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.1963  \u001b[0m | \u001b[0m 60.0    \u001b[0m | \u001b[0m 3.5     \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.439   \u001b[0m | \u001b[0m 0.2804  \u001b[0m | \u001b[0m 480.4   \u001b[0m | \u001b[0m 0.1958  \u001b[0m | \u001b[0m 0.06579 \u001b[0m | \u001b[0m 67.77   \u001b[0m | \u001b[0m 2.065   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 3.663   \u001b[0m |\n",
      "=================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "def action_with_warnings():\n",
    "    warnings.warn(\"should not appear\")\n",
    "\n",
    "with warnings.catch_warnings(record=True):\n",
    "    action_with_warnings()\n",
    "\n",
    "# Set paramaters\n",
    "params_nn ={\n",
    "    'neurons': (25, 60),\n",
    "    'activation':(0, 0.4),\n",
    "    'optimizer':(3.5,3.9),\n",
    "    'learning_rate':(0.01, 0.3),\n",
    "    'batch_size':(400, 500),\n",
    "    'epochs':(4, 72),\n",
    "    'layers1':(2,3),\n",
    "    'layers2':(2,3),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=100)\n",
    "nn_bo.maximize(init_points=30, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f390492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 419,\n",
       " 'dropout': 0.5218717978245897,\n",
       " 'dropout_rate': 0.14874659577110114,\n",
       " 'epochs': 71,\n",
       " 'layers1': 3,\n",
       " 'layers2': 2,\n",
       " 'learning_rate': 0.13680796401982245,\n",
       " 'neurons': 40,\n",
       " 'optimizer': <keras.optimizer_v2.adagrad.Adagrad at 0x1cb35e14ac0>}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential','relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "optimizerD= {'Adam':Adam(learning_rate=learning_rate), 'SGD':SGD(learning_rate=learning_rate),\n",
    "                 'RMSprop':RMSprop(learning_rate=learning_rate), 'Adadelta':Adadelta(learning_rate=learning_rate),\n",
    "                 'Adagrad':Adagrad(learning_rate=learning_rate), 'Adamax':Adamax(learning_rate=learning_rate),\n",
    "                 'Nadam':Nadam(learning_rate=learning_rate), 'Ftrl':Ftrl(learning_rate=learning_rate)}\n",
    "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d45986ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.4174 - accuracy: 0.3821\n",
      "Epoch 2/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3792 - accuracy: 0.3907\n",
      "Epoch 3/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3574 - accuracy: 0.4008\n",
      "Epoch 4/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3302 - accuracy: 0.4191\n",
      "Epoch 5/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.3152 - accuracy: 0.4279\n",
      "Epoch 6/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2969 - accuracy: 0.4322\n",
      "Epoch 7/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2902 - accuracy: 0.4358\n",
      "Epoch 8/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2883 - accuracy: 0.4335\n",
      "Epoch 9/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2710 - accuracy: 0.4466\n",
      "Epoch 10/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2661 - accuracy: 0.4433\n",
      "Epoch 11/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2574 - accuracy: 0.4456\n",
      "Epoch 12/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2613 - accuracy: 0.4417\n",
      "Epoch 13/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2567 - accuracy: 0.4408\n",
      "Epoch 14/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2595 - accuracy: 0.4486\n",
      "Epoch 15/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2570 - accuracy: 0.4537\n",
      "Epoch 16/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2420 - accuracy: 0.4552\n",
      "Epoch 17/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2525 - accuracy: 0.4549\n",
      "Epoch 18/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2660 - accuracy: 0.4458\n",
      "Epoch 19/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2427 - accuracy: 0.4574\n",
      "Epoch 20/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2375 - accuracy: 0.4539\n",
      "Epoch 21/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2453 - accuracy: 0.4532\n",
      "Epoch 22/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2493 - accuracy: 0.4514\n",
      "Epoch 23/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2218 - accuracy: 0.4608\n",
      "Epoch 24/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2363 - accuracy: 0.4544\n",
      "Epoch 25/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2281 - accuracy: 0.4518\n",
      "Epoch 26/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2251 - accuracy: 0.4623\n",
      "Epoch 27/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2287 - accuracy: 0.4636\n",
      "Epoch 28/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2212 - accuracy: 0.4638\n",
      "Epoch 29/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2176 - accuracy: 0.4649\n",
      "Epoch 30/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2193 - accuracy: 0.4664\n",
      "Epoch 31/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2226 - accuracy: 0.4597\n",
      "Epoch 32/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.1989 - accuracy: 0.4685\n",
      "Epoch 33/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2177 - accuracy: 0.4647\n",
      "Epoch 34/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2054 - accuracy: 0.4680\n",
      "Epoch 35/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2043 - accuracy: 0.4724\n",
      "Epoch 36/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.1965 - accuracy: 0.4698\n",
      "Epoch 37/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2033 - accuracy: 0.4708\n",
      "Epoch 38/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1892 - accuracy: 0.4748\n",
      "Epoch 39/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2025 - accuracy: 0.4739\n",
      "Epoch 40/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.1942 - accuracy: 0.4783\n",
      "Epoch 41/71\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2015 - accuracy: 0.4679\n",
      "Epoch 42/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1967 - accuracy: 0.4783\n",
      "Epoch 43/71\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1995 - accuracy: 0.4731\n",
      "Epoch 44/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1922 - accuracy: 0.4748\n",
      "Epoch 45/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1880 - accuracy: 0.4786\n",
      "Epoch 46/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1872 - accuracy: 0.4787\n",
      "Epoch 47/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1830 - accuracy: 0.4804\n",
      "Epoch 48/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1986 - accuracy: 0.4727\n",
      "Epoch 49/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1848 - accuracy: 0.4801\n",
      "Epoch 50/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1781 - accuracy: 0.4850\n",
      "Epoch 51/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1671 - accuracy: 0.4870\n",
      "Epoch 52/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1784 - accuracy: 0.4866\n",
      "Epoch 53/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1727 - accuracy: 0.4814\n",
      "Epoch 54/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1832 - accuracy: 0.4843\n",
      "Epoch 55/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1825 - accuracy: 0.4769\n",
      "Epoch 56/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1895 - accuracy: 0.4772\n",
      "Epoch 57/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1785 - accuracy: 0.4840\n",
      "Epoch 58/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1645 - accuracy: 0.4894\n",
      "Epoch 59/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1633 - accuracy: 0.4846\n",
      "Epoch 60/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1675 - accuracy: 0.4840\n",
      "Epoch 61/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1844 - accuracy: 0.4760\n",
      "Epoch 62/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1659 - accuracy: 0.4856\n",
      "Epoch 63/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1611 - accuracy: 0.4863\n",
      "Epoch 64/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1612 - accuracy: 0.4918\n",
      "Epoch 65/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1732 - accuracy: 0.4865\n",
      "Epoch 66/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1623 - accuracy: 0.4870\n",
      "Epoch 67/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1597 - accuracy: 0.4878\n",
      "Epoch 68/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1568 - accuracy: 0.4872\n",
      "Epoch 69/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1591 - accuracy: 0.4913\n",
      "Epoch 70/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1525 - accuracy: 0.4996\n",
      "Epoch 71/71\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1461 - accuracy: 0.4953\n",
      "218/218 - 1s - loss: 1.2247 - accuracy: 0.4545 - 556ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2246575355529785, 0.4544540345668793]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nn(**kwargs):\n",
    "    opt = kwargs['optimizer']\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(len(X.columns,))) # input with 934 \n",
    "    x = inputs\n",
    "    for i in range(kwargs['layers1']):\n",
    "        x = Dense(kwargs['neurons'], activation=kwargs['activation'])(x)\n",
    "    if kwargs['dropout'] < 0.5:\n",
    "        x = Dropout(kwargs['dropout_rate'], seed=123)(x)\n",
    "    for i in range(kwargs['layers2']):\n",
    "        x = Dense(kwargs['neurons'], activation=kwargs['activation'])(x)\n",
    "    outputs = Dense(5, activation='softmax')(x) #output\n",
    "    nn = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"base_model\")\n",
    "    nn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy']) \n",
    "    nn.fit(X_train.to_numpy(), y_train.to_numpy(), epochs = kwargs['epochs'],batch_size=kwargs['batch_size'])\n",
    "    return nn\n",
    "\n",
    "model = nn(**params_nn_)\n",
    "model.evaluate(X_test, y_test, verbose =2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6a2965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"../preprocessing/preprocesed_variables_test.csv\")\n",
    "og_test = pd.read_csv(\"../../data/test.csv\")\n",
    "unique_ids = og_test['uniqueid']\n",
    "\n",
    "data_test = data_test.drop(columns='health')\n",
    "data_test = data_test[important_variables['0']]\n",
    "\n",
    "test_pred_proba = model.predict_proba(data_test)\n",
    "test_pred = pd.DataFrame({'p':model.predict(data_test)})\n",
    "\n",
    "test_pred.to_csv(\"temp.csv\", index = False)\n",
    "preds = pd.DataFrame(test_pred_proba, columns=['p1','p2','p3','p4','p5'])\n",
    "preds.insert(0, 'uniqueid', unique_ids)\n",
    "\n",
    "preds.to_csv(\"xgb_pred.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
