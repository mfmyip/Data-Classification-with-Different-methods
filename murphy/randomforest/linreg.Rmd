---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r setup}
library(randomForest)
library(MASS)
library(caret)

setwd("E:/Google Drive/School/W2022/STAT 441/kaggle/stat441_w22_kaggle_project/murphy/randomforest")

X_train <- read.csv("TRAIN.csv")
X_test <- read.csv("TRAIN_VALID.csv")
TEST <- read.csv("TEST.csv")

y_test <- X_test['health']

cross_entropy <- function (predict, obs_class) {
  # predict is a matrix, obs_class vector of class {1..M}
  N <- NROW(predict)
  sum = 0
  for (i in 1:N) {
    sum = sum + log(predict[i, obs_class[i]])
  }
  return(-(sum/N))
}
```

```{r rm_corr}
tab <- read.csv("../../data/variables.csv")
tab <- tab[,c("var_code", "isTabulated")]
tab <- tab[as.logical(tab$isTabulated), 'var_code']
tab <- intersect(colnames(X_train), tab)

X_train[tab] <- lapply(X_train[tab], as.factor)
X_test[tab] <- lapply(X_test[tab], as.factor)
TEST[tab] <- lapply(X_test[tab], as.factor)
```

```{r model_fit, cache = T}
#Build ordinal logistic regression model
model <- randomForest(
  as.factor(health) ~ . - year, 
  data = X_train,
  ntree = 900,
  mtry = 5,
  maxnodes = 70
  )
print(model)
summary(model)
```

```{r var_analysis}
library(ggplot2)

# find important variables
imp <- importance(model)
imp_df <- data.frame(
  vars = rownames(imp),
  coef = imp
)
imp_df <- imp_df[rev(order(imp_df$MeanDecreaseGini)),]

#write.csv(imp_df, "importance.csv", row.names = F)

p<-ggplot(data=imp_df[1:50,], aes(x=reorder(var, MeanDecreaseGini), y=MeanDecreaseGini)) +
  geom_bar(stat="identity") +
  xlab("Predictor") +
  ylab("Importance (MeanDecreaseGini)") +
  coord_flip()
  
p
```

```{r predict}
p2 <- predict(model, X_test, type="prob")
cross_entropy(p2, y_test[[1]])

# transform predict test set
#var_subset <- colnames(train)[-NROW(colnames(train))]
#predict_set <- predict_set[, var_subset]
#predict_set[selected_factors[-10]] <- lapply(predict_set[selected_factors[-10]] , as.factor)
#predict_set <- data.frame(lapply(predict_set, function(x) {
#    if(is.factor(x)) replace(x, is.na(x), Mode(na.omit(x)))
#    else if(is.numeric(x)) replace(x, is.na(x), mean(x, na.rm=TRUE))
#    else x
#}))

prediction <- predict(model, predict_set, type = "prob")
out.df <- as.data.frame(prediction)
out.df <- cbind(predict_set$uniqueid, out.df)
colnames(out.df) <- c("uniqueid", "p1", "p2", "p3", "p4", "p5")
write.csv(out.df, "prediction.csv", row.names = F)
```

# attempt 2

Variable selection based on top 50% most important variables

```{r attempt2_setup}
pos <- with(imp_df, which.min(abs(imp_df$MeanDecreaseGini - median(imp_df$MeanDecreaseGini))))
selected <- c(imp_df[1:(pos-1),]$var, 'health')

train <- train[, selected]
predict_set <- predict_set[, selected[-length(selected)]]

#Random sampling 
samplesize = 0.70*nrow(train)
set.seed(100)
index = sample(seq_len(nrow(train)), size = samplesize)
#Creating training and test set 
datatrain = train[index,]
datatest = train[-index,]

model <- randomForest(health ~ . , data = datatrain, proximity = T)

p1 <- predict(model, datatest, type = "prob")
cross_entropy(p1, datatest$health)
```






